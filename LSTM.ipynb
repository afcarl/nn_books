{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will run through a simple LSTM in this section with keras, and have taken help from [here](http://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/)Â :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "import theano\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) (25000,)\n"
     ]
    }
   ],
   "source": [
    "# So, what's the shape here\n",
    "print (X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
      "1\n",
      "218 189\n"
     ]
    }
   ],
   "source": [
    "# That means we have 25000 data points, now lets see whats inside them\n",
    "print (X_train[0])\n",
    "print (y_train[0])\n",
    "print (len(X_train[0]), len(X_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234\n"
     ]
    }
   ],
   "source": [
    "# As we have preprocessed data it represents the sentence in vector form. (See we have sentences of different length)\n",
    "total_len = 0\n",
    "for i in X_train:\n",
    "    total_len += len(i)\n",
    "for i in X_test:\n",
    "    total_len += len(i)\n",
    "print (total_len // (X_train.shape[0] + X_test.shape[0])) # average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    1   14   22\n",
      "   16   43  530  973 1622 1385   65  458 4468   66 3941    4  173   36  256\n",
      "    5   25  100   43  838  112   50  670    2    9   35  480  284    5  150\n",
      "    4  172  112  167    2  336  385   39    4  172 4536 1111   17  546   38\n",
      "   13  447    4  192   50   16    6  147 2025   19   14   22    4 1920 4613\n",
      "  469    4   22   71   87   12   16   43  530   38   76   15   13 1247    4\n",
      "   22   17  515   17   12   16  626   18    2    5   62  386   12    8  316\n",
      "    8  106    5    4 2223    2   16  480   66 3785   33    4  130   12   16\n",
      "   38  619    5   25  124   51   36  135   48   25 1415   33    6   22   12\n",
      "  215   28   77   52    5   14  407   16   82    2    8    4  107  117    2\n",
      "   15  256    4    2    7 3766    5  723   36   71   43  530  476   26  400\n",
      "  317   46    7    4    2 1029   13  104   88    4  381   15  297   98   32\n",
      " 2071   56   26  141    6  194    2   18    4  226   22   21  134  476   26\n",
      "  480    5  144   30    2   18   51   36   28  224   92   25  104    4  226\n",
      "   65   16   38 1334   88   12   16  283    5   16 4472  113  103   32   15\n",
      "   16    2   19  178   32]\n",
      "500 500\n"
     ]
    }
   ],
   "source": [
    "# So lets pad or trim the sentences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "print (X_train[0])\n",
    "print (len(X_train[0]), len(X_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we have equal sized training data (Needed so for the LSTM)\n",
    "# create the model\n",
    "embedding_vector_length = 32\n",
    "\n",
    "# We are using a sequential model here, I will approach a functional model later\n",
    "model = Sequential()\n",
    "# We already have the sentences as vector representation, so why is the embedding used here ?\n",
    "# Because we don't need the distance between 'north' and 'south' same as 'modi' and 'kejriwal'. You can use GloVe here.\n",
    "# There are other reasons also.\n",
    "embedding = Embedding(top_words, embedding_vector_length, input_length=max_review_length)\n",
    "model.add(embedding)\n",
    "# LSTM of 100 hidden units (more on it later)\n",
    "lstm = LSTM(100)\n",
    "model.add(lstm)\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Here are the functions to visualize the model.\n",
    "embedding_out = theano.function([model.input], [embedding.output])\n",
    "lstm_out = theano.function([model.input], [lstm.output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (1, 500, 32) [array([[[-0.01909696, -0.00128354, -0.04350078, ...,  0.04515557,\n",
      "         -0.04617136, -0.03478462],\n",
      "        [-0.01909696, -0.00128354, -0.04350078, ...,  0.04515557,\n",
      "         -0.04617136, -0.03478462],\n",
      "        [-0.01909696, -0.00128354, -0.04350078, ...,  0.04515557,\n",
      "         -0.04617136, -0.03478462],\n",
      "        ..., \n",
      "        [ 0.02845694,  0.01439202, -0.04001575, ...,  0.04781301,\n",
      "         -0.04650199, -0.01355803],\n",
      "        [ 0.01874172,  0.03280069,  0.00071394, ...,  0.01790861,\n",
      "         -0.00488006, -0.00624097],\n",
      "        [ 0.04649385,  0.01548138,  0.00673046, ..., -0.0140023 ,\n",
      "         -0.02662515,  0.03282387]]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "e_out = embedding_out([X_train[0]])\n",
    "print (len(e_out), e_out[0].shape, e_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100) [array([[ -8.75755120e-03,   2.35159490e-02,   7.24158762e-03,\n",
      "          5.75434649e-03,   7.76602514e-03,   6.44575851e-03,\n",
      "         -1.07395388e-02,  -1.99354794e-02,  -1.67083833e-02,\n",
      "          1.39765628e-02,  -9.82308853e-03,   1.44931360e-03,\n",
      "          1.96614452e-02,   1.79646432e-03,  -3.28742759e-03,\n",
      "         -7.29508698e-04,   1.29413130e-02,  -2.22204439e-03,\n",
      "         -8.85574613e-03,  -7.34617747e-03,  -2.32800539e-03,\n",
      "         -3.01935384e-03,  -1.22768572e-02,   2.00342480e-02,\n",
      "         -1.16039836e-03,   1.11591378e-02,   4.85542044e-03,\n",
      "          1.24953324e-02,   1.13097318e-02,   1.64835844e-02,\n",
      "         -1.26700439e-02,  -1.70683097e-02,   3.61562381e-03,\n",
      "         -8.89317039e-03,  -9.64088645e-03,   7.36068236e-03,\n",
      "          5.49160503e-03,   2.23985426e-02,  -9.45511088e-03,\n",
      "          7.04189064e-03,  -3.34612802e-02,   2.03254614e-02,\n",
      "         -2.15605870e-02,  -2.68418971e-03,  -7.75714964e-03,\n",
      "          5.58061386e-03,  -1.00356992e-02,  -1.92607623e-02,\n",
      "         -1.36646675e-03,   8.06782674e-03,   4.58518788e-02,\n",
      "         -3.53545393e-03,  -4.56694001e-03,   4.63988772e-03,\n",
      "          2.84212967e-03,  -4.02517710e-03,  -9.18996893e-03,\n",
      "          5.26939891e-03,  -6.42416673e-03,  -3.71117749e-05,\n",
      "          4.37601190e-03,   2.05654022e-03,  -3.20898648e-03,\n",
      "         -2.56315954e-02,   1.28993089e-03,   5.59492316e-03,\n",
      "          6.77664392e-03,   7.22062017e-04,   3.33167543e-03,\n",
      "          6.88908680e-04,   3.23175598e-04,   6.44317083e-03,\n",
      "          6.13973709e-03,  -1.15033416e-02,  -5.73192583e-03,\n",
      "          3.00405268e-02,   1.14328042e-02,  -1.08586764e-02,\n",
      "         -1.56549539e-03,   1.45792356e-02,   8.82619061e-03,\n",
      "          6.90228213e-03,  -2.03497428e-03,  -1.29626831e-02,\n",
      "          3.35962186e-03,  -8.66493583e-03,   2.37961374e-02,\n",
      "          2.17049150e-03,   1.65924467e-02,  -8.55523720e-03,\n",
      "          4.53922432e-03,   1.71336979e-02,  -1.03594363e-02,\n",
      "          4.25248640e-03,  -2.20666528e-02,   2.04517785e-02,\n",
      "          8.50998610e-03,  -2.87471921e-03,   4.10225336e-03,\n",
      "         -6.87998626e-03]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# We see that we are giving a input of `input_length = max_review_length` (that's 500 = len(X_train[0]))\n",
    "# The embedding layer has:\n",
    "#   first_argument = input_dim = top_words = 5000 (No value in input should be greater than this)\n",
    "#   second_argument = output_dim = 32\n",
    "# More here: https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "# So we get output which is of 1 dimension (denotes number of epoch), which in turn is of\n",
    "# 500 dimensions (number of words) which in turn are of 32 dimensions (output_dime)\n",
    "l_out = lstm_out([X_train[0]])\n",
    "print (l_out[0].shape, l_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_8 (Embedding)          (None, 500, 32)       160000      embedding_input_8[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                    (None, 100)           53200       embedding_8[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 1)             101         lstm_8[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 213301\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# In LSTM, the first argument is the output dimension\n",
    "# So we get output which is of 1 dimension (denotes number of epoch), which is of 100 dimensions (output_dim)\n",
    "print(model.summary())\n",
    "# The dense layer is easy to understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 451s - loss: 0.4845 - acc: 0.7606 - val_loss: 0.3351 - val_acc: 0.8647\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 559s - loss: 0.2961 - acc: 0.8824 - val_loss: 0.3130 - val_acc: 0.8700\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 499s - loss: 0.2891 - acc: 0.8837 - val_loss: 0.3223 - val_acc: 0.8661\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fed112617b8>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=3, batch_size=64, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.61%\n"
     ]
    }
   ],
   "source": [
    "# Well we have used the same validation data and test data\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
